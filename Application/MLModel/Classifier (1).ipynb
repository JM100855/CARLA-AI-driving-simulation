{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JCya32yI9mHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca57641-2bb8-4983-a4f1-b721382875e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "6bjMMZnZBExG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "xP2KxAEN-qWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3753428e-f26d-4cad-c750-397c6c8dda16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/classifier/cnnmodel.h5\""
      ],
      "metadata": {
        "id": "MYJywMJt-rB9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "LHUcnTv1AaEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8cfc42c-b5d4-43fa-f84e-8e23f26673cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# --- your image path ---\n",
        "img_path = \"/content/drive/MyDrive/classifier/A0767(0.7011).png\"\n",
        "\n",
        "# --- load image and basic check ---\n",
        "img = cv2.imread(img_path)\n",
        "if img is None:\n",
        "    raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "\n",
        "# --- inspect model input shape ---\n",
        "# model should already be loaded in your session\n",
        "try:\n",
        "    in_shape = model.input_shape  # e.g. (None, H, W, C)\n",
        "except Exception:\n",
        "    # fallback\n",
        "    in_shape = model.layers[0].input_shape\n",
        "\n",
        "if len(in_shape) != 4:\n",
        "    raise ValueError(f\"Unsupported model input shape: {in_shape}\")\n",
        "\n",
        "_, H, W, C = in_shape\n",
        "H = int(H) if in_shape[1] is not None else 224\n",
        "W = int(W) if in_shape[2] is not None else 224\n",
        "C = int(C)\n",
        "\n",
        "print(f\"Model expects: {H}x{W}x{C}\")\n",
        "\n",
        "# --- preprocess according to channels ---\n",
        "# If model expects grayscale (C==1) convert to gray then expand channel dim\n",
        "if C == 1:\n",
        "    # convert BGR -> GRAY\n",
        "    img_proc = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img_proc = cv2.resize(img_proc, (W, H))\n",
        "    # add channel axis: (H,W) -> (H,W,1)\n",
        "    img_proc = np.expand_dims(img_proc, axis=-1)\n",
        "elif C == 3:\n",
        "    # convert BGR -> RGB and resize\n",
        "    img_proc = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img_proc = cv2.resize(img_proc, (W, H))\n",
        "else:\n",
        "    raise ValueError(f\"Model expects {C} channels ‚Äî this code only handles 1 or 3.\")\n",
        "\n",
        "# --- normalize and batchify ---\n",
        "x = img_proc.astype(\"float32\") / 255.0\n",
        "x = np.expand_dims(x, axis=0)   # (1, H, W, C)\n",
        "\n",
        "# --- predict ---\n",
        "pred = model.predict(x)\n",
        "\n",
        "# handle sigmoid or softmax outputs robustly\n",
        "pred = np.array(pred)\n",
        "if pred.ndim == 2 and pred.shape[1] == 1:\n",
        "    prob = float(pred[0,0])\n",
        "elif pred.ndim == 2 and pred.shape[1] >= 2:\n",
        "    prob = float(pred[0,1])   # class-1 prob\n",
        "else:\n",
        "    prob = float(np.ravel(pred)[0])\n",
        "\n",
        "prob = float(prob)  # just making sure\n",
        "state = \"\"\n",
        "\n",
        "if prob < 0.4:\n",
        "    state = \"alert\"\n",
        "elif prob < 0.7:\n",
        "    state = \"slightly drowsy\"\n",
        "elif prob < 0.85:\n",
        "    state = \"very drowsy\"\n",
        "else:\n",
        "    state = \"critical drowsiness\"\n",
        "\n",
        "print(f\"Prediction probability: {prob:.4f}\")\n",
        "print(f\"Predicted class: {state} (0-0.4 = alert, 0.4-0.7 = slightly drowsy,0.7-0.85 = very drowsy, 0.85-1 = critical drowsiness)\")\n",
        "\n",
        "# label = int(prob > 0.5)\n",
        "\n",
        "# print(f\"Prediction probability: {prob:.4f}\")\n",
        "# print(f\"Predicted class: {label} (1 = drowsy, 0 = alert)\")\n"
      ],
      "metadata": {
        "id": "cZbpSBJ-CLMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d4e4a4-87f5-4568-bf96-cdc0580f79fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model expects: 100x100x1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step\n",
            "Prediction probability: 0.7011\n",
            "Predicted class: very drowsy (0-0.4 = alert, 0.4-0.7 = slightly drowsy,0.7-0.85 = very drowsy, 0.85-1 = critical drowsiness)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "\n",
        "# Load face detector and landmarks predictor\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"/content/drive/MyDrive/classifier/shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "# Feature calculation functions (EAR, PUC, MAR, MOE)\n",
        "def eye_aspect_ratio(eye):\n",
        "    x = [point.x for point in eye]\n",
        "    y = [point.y for point in eye]\n",
        "    A = np.linalg.norm(np.array([x[1]-x[5], y[1]-y[5]]))\n",
        "    B = np.linalg.norm(np.array([x[2]-x[4], y[2]-y[4]]))\n",
        "    C = np.linalg.norm(np.array([x[0]-x[3], y[0]-y[3]]))\n",
        "    return (A + B) / (2.0 * C)\n",
        "\n",
        "def pupil_to_eye_center_distance(eye):\n",
        "    x = [point.x for point in eye]\n",
        "    y = [point.y for point in eye]\n",
        "    return np.linalg.norm(np.array([x[0]-x[3], y[0]-y[3]]))\n",
        "\n",
        "def mouth_aspect_ratio(mouth):\n",
        "    x = [point.x for point in mouth]\n",
        "    y = [point.y for point in mouth]\n",
        "    A = np.linalg.norm(np.array([x[13]-x[19], y[13]-y[19]]))\n",
        "    B = np.linalg.norm(np.array([x[14]-x[18], y[14]-y[18]]))\n",
        "    C = np.linalg.norm(np.array([x[15]-x[17], y[15]-y[17]]))\n",
        "    return (A + B + C) / (3.0 * np.linalg.norm(np.array([x[12]-x[16], y[12]-y[16]])))\n",
        "\n",
        "def mouth_to_eye_ratio(eye, mouth):\n",
        "    ear = eye_aspect_ratio(eye)\n",
        "    mar = mouth_aspect_ratio(mouth)\n",
        "    if ear == 0:  # avoid division by zero\n",
        "        return 0\n",
        "    return mar / ear\n",
        "\n",
        "# Function to extract features from a single image\n",
        "def extract_features_from_image(frame):\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        return None  # no face detected\n",
        "\n",
        "    face = faces[0]  # take the first detected face\n",
        "    shape = predictor(gray, face)\n",
        "\n",
        "    ear = eye_aspect_ratio(shape.parts()[36:42])\n",
        "    puc = pupil_to_eye_center_distance(shape.parts()[36:42])\n",
        "    mar = mouth_aspect_ratio(shape.parts()[48:68])\n",
        "    moe = mouth_to_eye_ratio(shape.parts()[36:42], shape.parts()[48:68])\n",
        "\n",
        "    # Return as numpy array shaped (1,2,2,1) for your CNN\n",
        "    feature_vector = np.array([[ear, puc], [mar, moe]]).reshape(1, 2, 2, 1).astype(np.float32)\n",
        "    return feature_vector"
      ],
      "metadata": {
        "id": "0yPrW0MmVdAV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(img_path)\n",
        "\n",
        "features = extract_features_from_image(img)\n",
        "print(\"Output of extract_features_from_image():\")\n",
        "print(\"Type:\", type(features))\n",
        "print(\"Value:\", features.shape)"
      ],
      "metadata": {
        "id": "It8KhrGijr41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5eb29f-318c-44da-fb19-71bc65af4dd7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of extract_features_from_image():\n",
            "Type: <class 'numpy.ndarray'>\n",
            "Value: (1, 2, 2, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- paste this entire block into a notebook cell ----\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ---------------- User paths / model in session ----------------\n",
        "# Ensure you already have `model` loaded in the kernel (tf.keras.Model).\n",
        "# If not, load it before calling the functions below.\n",
        "PREDICTOR_PATH = \"/content/drive/MyDrive/classifier/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "# ---------------- dlib detector / predictor ----------------\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(PREDICTOR_PATH)\n",
        "\n",
        "# ---------------- Feature extractor (returns shape (4,)) ----------------\n",
        "def eye_aspect_ratio(eye_points):\n",
        "    coords = np.array([[p.x, p.y] for p in eye_points], dtype=np.float32)\n",
        "    A = np.linalg.norm(coords[1] - coords[5])\n",
        "    B = np.linalg.norm(coords[2] - coords[4])\n",
        "    C = np.linalg.norm(coords[0] - coords[3])\n",
        "    if C == 0:\n",
        "        return 0.0\n",
        "    return float((A + B) / (2.0 * C))\n",
        "\n",
        "def pupil_to_eye_center_distance(eye_points):\n",
        "    coords = np.array([[p.x, p.y] for p in eye_points], dtype=np.float32)\n",
        "    centroid = coords.mean(axis=0)\n",
        "    corner_midpoint = (coords[0] + coords[3]) / 2.0\n",
        "    return float(np.linalg.norm(centroid - corner_midpoint))\n",
        "\n",
        "def mouth_aspect_ratio(mouth_points):\n",
        "    coords = np.array([[p.x, p.y] for p in mouth_points], dtype=np.float32)\n",
        "    # indices relative to mouth_points (20 points)\n",
        "    # choose three vertical distances and horizontal width\n",
        "    try:\n",
        "        A = np.linalg.norm(coords[13] - coords[19])\n",
        "        B = np.linalg.norm(coords[14] - coords[18])\n",
        "        C = np.linalg.norm(coords[15] - coords[17])\n",
        "        horizontal = np.linalg.norm(coords[12] - coords[16])\n",
        "    except Exception:\n",
        "        # fallback if geometry unexpected\n",
        "        return 0.0\n",
        "    if horizontal == 0:\n",
        "        return 0.0\n",
        "    return float((A + B + C) / (3.0 * horizontal))\n",
        "\n",
        "def extract_features_from_image(frame):\n",
        "    \"\"\"\n",
        "    Robust feature extractor for use with finite-difference perturbations.\n",
        "    Accepts numpy arrays of any numeric dtype (float32/float64/int), clips to 0..255,\n",
        "    converts to uint8 as required by OpenCV/dlib, and returns np.array([EAR,PUC,MAR,MOE])\n",
        "    or None if no face detected.\n",
        "    \"\"\"\n",
        "    if frame is None:\n",
        "        return None\n",
        "\n",
        "    # Ensure numeric array\n",
        "    img = np.asarray(frame)\n",
        "\n",
        "    # If image is in range [0,1], scale up to [0,255] (heuristic)\n",
        "    if img.dtype.kind == 'f':\n",
        "        # If floats appear to be in 0..1, scale; otherwise assume 0..255 but still clip\n",
        "        if img.max() <= 1.0:\n",
        "            img = (img * 255.0).astype(np.float32)\n",
        "        # now clip and cast to uint8\n",
        "        img = np.clip(img, 0.0, 255.0).astype(np.uint8)\n",
        "    else:\n",
        "        # integer types: clip to valid 0..255 and cast to uint8\n",
        "        img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # If single-channel (H,W), keep as is for detector; if 3-channel, assume BGR (OpenCV)\n",
        "    if img.ndim == 2:\n",
        "        gray = img\n",
        "    elif img.ndim == 3 and img.shape[2] == 3:\n",
        "        # already BGR uint8 as expected\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        # unexpected channel count: try to reduce/expand to 3 channels\n",
        "        if img.ndim == 3 and img.shape[2] == 4:\n",
        "            # drop alpha\n",
        "            img = img[..., :3]\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        elif img.ndim == 3 and img.shape[2] == 1:\n",
        "            gray = img[..., 0]\n",
        "        else:\n",
        "            # fallback: convert to grayscale using mean across channels\n",
        "            gray = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Run detector on uint8 grayscale\n",
        "    faces = detector(gray)\n",
        "    if len(faces) == 0:\n",
        "        return None\n",
        "\n",
        "    face = faces[0]\n",
        "    shape = predictor(gray, face)\n",
        "    parts = list(shape.parts())\n",
        "\n",
        "    # Safety: ensure we have 68 landmarks\n",
        "    if len(parts) < 68:\n",
        "        return None\n",
        "\n",
        "    # Landmarks slices (dlib 68-point mapping)\n",
        "    left_eye = parts[36:42]\n",
        "    right_eye = parts[42:48]\n",
        "    mouth = parts[48:68]\n",
        "\n",
        "    # Helper local functions (same as earlier definitions)\n",
        "    def eye_aspect_ratio_local(eye_points):\n",
        "        coords = np.array([[p.x, p.y] for p in eye_points], dtype=np.float32)\n",
        "        A = np.linalg.norm(coords[1] - coords[5])\n",
        "        B = np.linalg.norm(coords[2] - coords[4])\n",
        "        C = np.linalg.norm(coords[0] - coords[3])\n",
        "        if C == 0:\n",
        "            return 0.0\n",
        "        return float((A + B) / (2.0 * C))\n",
        "\n",
        "    def pupil_to_eye_center_distance_local(eye_points):\n",
        "        coords = np.array([[p.x, p.y] for p in eye_points], dtype=np.float32)\n",
        "        centroid = coords.mean(axis=0)\n",
        "        corner_midpoint = (coords[0] + coords[3]) / 2.0\n",
        "        return float(np.linalg.norm(centroid - corner_midpoint))\n",
        "\n",
        "    def mouth_aspect_ratio_local(mouth_points):\n",
        "        coords = np.array([[p.x, p.y] for p in mouth_points], dtype=np.float32)\n",
        "        try:\n",
        "            A = np.linalg.norm(coords[13] - coords[19])\n",
        "            B = np.linalg.norm(coords[14] - coords[18])\n",
        "            C = np.linalg.norm(coords[15] - coords[17])\n",
        "            horizontal = np.linalg.norm(coords[12] - coords[16])\n",
        "        except Exception:\n",
        "            return 0.0\n",
        "        if horizontal == 0:\n",
        "            return 0.0\n",
        "        return float((A + B + C) / (3.0 * horizontal))\n",
        "\n",
        "    left_ear = eye_aspect_ratio_local(left_eye)\n",
        "    right_ear = eye_aspect_ratio_local(right_eye)\n",
        "    ear = float((left_ear + right_ear) / 2.0)\n",
        "\n",
        "    left_puc = pupil_to_eye_center_distance_local(left_eye)\n",
        "    right_puc = pupil_to_eye_center_distance_local(right_eye)\n",
        "    puc = float((left_puc + right_puc) / 2.0)\n",
        "\n",
        "    mar = mouth_aspect_ratio_local(mouth)\n",
        "    moe = float(mar / (ear if ear != 0 else 1e-6))\n",
        "\n",
        "    return np.array([ear, puc, mar, moe], dtype=np.float32)\n",
        "\n",
        "\n",
        "# ---------------- Feature importance routine ----------------\n",
        "def _to_float_img(img):\n",
        "    # keep original pixel units (0..255) for finite differences\n",
        "    if img.dtype == np.uint8:\n",
        "        return img.astype(np.float32)\n",
        "    return img.astype(np.float32)\n",
        "\n",
        "def _infer_model_input_shape(model):\n",
        "    # tries to get (H,W,C) from model.input_shape\n",
        "    try:\n",
        "        s = model.input_shape\n",
        "    except Exception:\n",
        "        # fallback to first layer\n",
        "        s = model.layers[0].input_shape\n",
        "    # Expect formats like (None,H,W,C)\n",
        "    if isinstance(s, tuple) and len(s) >= 4:\n",
        "        H = int(s[1]) if s[1] is not None else None\n",
        "        W = int(s[2]) if s[2] is not None else None\n",
        "        C = int(s[3]) if s[3] is not None else None\n",
        "        return (H, W, C)\n",
        "    # If channels-first (None,C,H,W)\n",
        "    if isinstance(s, tuple) and len(s) >= 4:\n",
        "        C = int(s[1]) if s[1] is not None else None\n",
        "        H = int(s[2]) if s[2] is not None else None\n",
        "        W = int(s[3]) if s[3] is not None else None\n",
        "        return (H, W, C)\n",
        "    raise RuntimeError(\"Unable to infer model input shape from model.input_shape.\")\n",
        "\n",
        "def _default_preprocess(img, target_hw=None, target_c=None):\n",
        "    \"\"\"\n",
        "    Converts BGR OpenCV image to model input tensor:\n",
        "    - if model expects 3 channels: convert to RGB\n",
        "    - if model expects 1 channel: convert to grayscale\n",
        "    - resizes to target_hw if provided\n",
        "    - scales to [0,1] (divides by 255.0)\n",
        "    Returns tf.Tensor shape (1,H,W,C) dtype float32 and scale factor (how model_input = pixel * scale)\n",
        "    \"\"\"\n",
        "    imgf = _to_float_img(img)  # pixel units 0..255\n",
        "    if target_hw is not None:\n",
        "        h, w = target_hw\n",
        "        imgf = cv2.resize(imgf, (w, h))\n",
        "    cur_c = imgf.shape[2] if imgf.ndim == 3 else 1\n",
        "    if target_c == 1 and cur_c == 3:\n",
        "        img_proc = cv2.cvtColor(imgf.astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "        img_proc = np.expand_dims(img_proc.astype(np.float32), axis=-1)\n",
        "    elif target_c == 3 and cur_c == 1:\n",
        "        img_proc = np.repeat(imgf, 3, axis=2)\n",
        "    elif target_c == 3 and cur_c == 3:\n",
        "        img_proc = cv2.cvtColor(imgf.astype(np.uint8), cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    else:\n",
        "        # fallback: keep as-is\n",
        "        img_proc = imgf\n",
        "    # scale to [0,1]\n",
        "    model_input = img_proc.astype(np.float32) / 255.0\n",
        "    return tf.convert_to_tensor(model_input[None, ...], dtype=tf.float32), (1.0 / 255.0)  # scale = d(model_input)/d(pixel)\n",
        "\n",
        "def feature_importances_via_patch_fd_v2(model,\n",
        "                                        orig_img,\n",
        "                                        extract_features_fn,\n",
        "                                        preprocess_for_model=None,\n",
        "                                        patch_size=16,\n",
        "                                        eps=4.0,\n",
        "                                        feature_names=['EAR','PUC','MAR','MOE'],\n",
        "                                        target_class=None):\n",
        "    \"\"\"\n",
        "    model: TF model\n",
        "    orig_img: original HxWxC BGR image in pixel units (0..255) as numpy array\n",
        "    extract_features_fn: function(frame)->np.array([4,]) operating on images in same format as orig_img\n",
        "    preprocess_for_model: optional function(img)->(tensor shape (1,H,W,C), scale) where scale = d(model_input)/d(pixel)\n",
        "                          If None, default preprocess (resize & /255.0) is used.\n",
        "    patch_size: patch side in pixels for finite differences\n",
        "    eps: finite-diff amplitude in pixel units (0..255). e.g., 4.0\n",
        "    \"\"\"\n",
        "    img_pixels = _to_float_img(orig_img)\n",
        "    H_img, W_img = img_pixels.shape[0], img_pixels.shape[1]\n",
        "\n",
        "    # Determine model input shape and prepare model_input tensor\n",
        "    H_model, W_model, C_model = _infer_model_input_shape(model)\n",
        "    target_hw = (H_model, W_model) if H_model is not None and W_model is not None else None\n",
        "\n",
        "    if preprocess_for_model is None:\n",
        "        model_input_tensor, scale = _default_preprocess(img_pixels, target_hw=target_hw, target_c=C_model)\n",
        "    else:\n",
        "        # preprocess_for_model should return (tensor, scale) OR just tensor (then we assume scale=1/255)\n",
        "        res = preprocess_for_model(img_pixels)\n",
        "        if isinstance(res, tuple) and len(res) == 2:\n",
        "            model_input_tensor, scale = res\n",
        "        else:\n",
        "            model_input_tensor = res\n",
        "            # assume normalized /255\n",
        "            scale = (1.0 / 255.0)\n",
        "\n",
        "    # Ensure model_input_tensor is tf.Variable for gradient computation\n",
        "    img_tf_var = tf.Variable(model_input_tensor)\n",
        "\n",
        "    # Compute gradient dY/d(model_input)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(img_tf_var)\n",
        "        preds = model(img_tf_var)\n",
        "        preds = tf.convert_to_tensor(preds)\n",
        "        # turn preds into scalar\n",
        "        if preds.shape.rank == 1 or (preds.shape.rank == 2 and preds.shape[1] == 1):\n",
        "            out_scalar = tf.reshape(preds, [preds.shape[0]])[0]\n",
        "        else:\n",
        "            if target_class is not None:\n",
        "                out_scalar = tf.reshape(preds[:, target_class], [])\n",
        "            else:\n",
        "                # use predicted class score\n",
        "                argmax = tf.argmax(preds, axis=1)[0]\n",
        "                out_scalar = tf.reshape(preds[0, argmax], [])\n",
        "    grad_dy_dmodel = tape.gradient(out_scalar, img_tf_var)\n",
        "    if grad_dy_dmodel is None:\n",
        "        raise RuntimeError(\"Gradient is None. Check model and preprocess.\")\n",
        "\n",
        "    # grad_dy_dmodel has same shape as model_input (1,Hm,Wm,Cm)\n",
        "    grad_dy_dmodel = grad_dy_dmodel.numpy()[0]  # Hm x Wm x Cm\n",
        "\n",
        "    # Convert gradient to pixel units: dY/d(pixel) = dY/d(model_input) * d(model_input)/d(pixel)\n",
        "    # scale = d(model_input)/d(pixel) (e.g., 1/255), so multiply\n",
        "    grad_dy_dpixel = grad_dy_dmodel * scale  # now expresses change in output per 1 pixel unit change\n",
        "\n",
        "    # Compute base features on original image (pixel-domain)\n",
        "    base_features = extract_features_fn(img_pixels)\n",
        "    if base_features is None:\n",
        "        raise RuntimeError(\"No face/features detected in the original image.\")\n",
        "    if base_features.shape[0] != len(feature_names):\n",
        "        raise ValueError(\"extract_features_fn must return len(feature_names) features.\")\n",
        "\n",
        "    # We'll compute dF/dI (per-pixel per-channel in pixel units) by patch finite differences on original image\n",
        "    n_patches_h = int(np.ceil(H_img / patch_size))\n",
        "    n_patches_w = int(np.ceil(W_img / patch_size))\n",
        "\n",
        "    # dF_dI needs to be in model-input channel-space (Cm). We'll map later.\n",
        "    # First compute per-pixel per-original-channel derivatives (orig has C_orig channels)\n",
        "    C_orig = img_pixels.shape[2]\n",
        "    # We'll compute dF/dI in the original image channels, then map to model channels.\n",
        "    dF_dI_orig = np.zeros((len(feature_names), H_img, W_img, C_orig), dtype=np.float32)\n",
        "\n",
        "    for ph in range(n_patches_h):\n",
        "        for pw in range(n_patches_w):\n",
        "            y0 = ph * patch_size\n",
        "            x0 = pw * patch_size\n",
        "            y1 = min(H_img, y0 + patch_size)\n",
        "            x1 = min(W_img, x0 + patch_size)\n",
        "            plus_img = img_pixels.copy()\n",
        "            minus_img = img_pixels.copy()\n",
        "\n",
        "            plus_img[y0:y1, x0:x1, :] += eps\n",
        "            minus_img[y0:y1, x0:x1, :] -= eps\n",
        "\n",
        "            plus_img = np.clip(plus_img, 0.0, 255.0)\n",
        "            minus_img = np.clip(minus_img, 0.0, 255.0)\n",
        "\n",
        "            f_plus = extract_features_fn(plus_img)\n",
        "            f_minus = extract_features_fn(minus_img)\n",
        "            if f_plus is None or f_minus is None:\n",
        "                # skip patch if features can't be computed\n",
        "                continue\n",
        "            df_patch = (f_plus - f_minus) / (2.0 * eps)  # shape (4,)\n",
        "            patch_area = float((y1 - y0) * (x1 - x0))\n",
        "            if patch_area <= 0:\n",
        "                continue\n",
        "            per_pixel_df = (df_patch / patch_area).reshape(len(feature_names), 1, 1)  # per-pixel per-channel scalar (applied equally to channels)\n",
        "\n",
        "            # assign to each original channel equally (we perturbed all channels at once)\n",
        "            for i in range(len(feature_names)):\n",
        "                # assume per-channel effect is same across C_orig when we added same eps to all channels\n",
        "                dF_dI_orig[i, y0:y1, x0:x1, :] = per_pixel_df[i]\n",
        "\n",
        "    # Now map dF_dI_orig (orig channels) into model-input channel space (Cm).\n",
        "    # Strategy:\n",
        "    #  - If Cm == C_orig: direct mapping by resizing to model-input resolution if needed.\n",
        "    #  - If model input is resized (Hm,Wm) != original (H_img,W_img): resize each channel map to Hm x Wm.\n",
        "    #  - If Cm == 1 and C_orig == 3: collapse channels (sum or weighted)\n",
        "    #  - If Cm == 3 and C_orig == 1: replicate across channels\n",
        "    Hm = grad_dy_dpixel.shape[0]\n",
        "    Wm = grad_dy_dpixel.shape[1]\n",
        "    Cm = grad_dy_dpixel.shape[2]\n",
        "\n",
        "    dF_dI_modelspace = np.zeros((len(feature_names), Hm, Wm, Cm), dtype=np.float32)\n",
        "\n",
        "    for i in range(len(feature_names)):\n",
        "        # for each original channel, resize its HxW map to Hm x Wm\n",
        "        # then map channels\n",
        "        # build temp array shape (Hm, Wm, C_orig)\n",
        "        resized_channels = []\n",
        "        for ch in range(C_orig):\n",
        "            channel_map = dF_dI_orig[i, :, :, ch].astype(np.float32)\n",
        "            # resize: cv2.resize expects (w,h)\n",
        "            resized = cv2.resize(channel_map, (Wm, Hm), interpolation=cv2.INTER_LINEAR)\n",
        "            resized_channels.append(resized)\n",
        "        resized_channels = np.stack(resized_channels, axis=-1)  # Hm x Wm x C_orig\n",
        "\n",
        "        if Cm == C_orig:\n",
        "            dF_dI_modelspace[i] = resized_channels\n",
        "        elif Cm == 1 and C_orig == 3:\n",
        "            # collapse RGB by summation (we perturbed all channels equally so collapse by mean)\n",
        "            collapsed = np.mean(resized_channels, axis=-1)\n",
        "            dF_dI_modelspace[i, :, :, 0] = collapsed\n",
        "        elif Cm == 3 and C_orig == 1:\n",
        "            # replicate grayscale to 3-channels\n",
        "            for ch in range(3):\n",
        "                dF_dI_modelspace[i, :, :, ch] = resized_channels[:, :, 0]\n",
        "        else:\n",
        "            # fallback: if channels differ, distribute equally across Cm\n",
        "            for ch in range(Cm):\n",
        "                dF_dI_modelspace[i, :, :, ch] = np.mean(resized_channels, axis=-1) / float(Cm)\n",
        "\n",
        "    # Finally, compute dY/dF_i = sum_{pixels,channels} dY/d(pixel) * dF/d(pixel)\n",
        "    importances_raw = {}\n",
        "    for i, name in enumerate(feature_names):\n",
        "        dot = float(np.sum(grad_dy_dpixel * dF_dI_modelspace[i]))\n",
        "        importances_raw[name] = dot\n",
        "\n",
        "    abs_vals = {k: abs(v) for k, v in importances_raw.items()}\n",
        "    total = sum(abs_vals.values()) + 1e-12\n",
        "    normalized = {k: abs_vals[k] / total for k in feature_names}\n",
        "\n",
        "    return {\n",
        "        \"raw\": importances_raw,         # signed sensitivities\n",
        "        \"magnitude\": abs_vals,\n",
        "        \"normalized\": normalized,\n",
        "        \"base_features\": base_features\n",
        "    }\n",
        "\n",
        "# ---------------- Example usage ----------------\n",
        "# Make sure `model` is defined in your notebook (tf.keras.Model).\n",
        "# Make sure you have an image at IMG_PATH.\n",
        "img = cv2.imread(img_path)\n",
        "if img is None:\n",
        "    raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "\n",
        "# Run feature importance\n",
        "result = feature_importances_via_patch_fd_v2(model, img, extract_features_from_image,\n",
        "                                            preprocess_for_model=None,\n",
        "                                            patch_size=16, eps=4.0,\n",
        "                                            feature_names=['EAR','PUC','MAR','MOE'],\n",
        "                                            target_class=None)\n",
        "\n",
        "print(\"Base features:\", result['base_features'])\n",
        "print(\"Raw dY/dF:\", result['raw'])\n",
        "print(\"Magnitude:\", result['magnitude'])\n",
        "print(\"Normalized:\", result['normalized'])\n",
        "print(result)\n",
        "# --------------------------------------------------\n"
      ],
      "metadata": {
        "id": "FepMmoM2-dkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "badb1891-539c-4191-f3ce-84ee90933bcb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base features: [0.2252123  0.82143927 0.02066149 0.09174228]\n",
            "Raw dY/dF: {'EAR': 4.784571387972392e-07, 'PUC': 2.892115844588261e-06, 'MAR': 5.346513987092294e-08, 'MOE': 1.3120636310759437e-07}\n",
            "Magnitude: {'EAR': 4.784571387972392e-07, 'PUC': 2.892115844588261e-06, 'MAR': 5.346513987092294e-08, 'MOE': 1.3120636310759437e-07}\n",
            "Normalized: {'EAR': 0.13457780640812, 'PUC': 0.8134785222794995, 'MAR': 0.015038382040279935, 'MOE': 0.03690500799757159}\n",
            "{'raw': {'EAR': 4.784571387972392e-07, 'PUC': 2.892115844588261e-06, 'MAR': 5.346513987092294e-08, 'MOE': 1.3120636310759437e-07}, 'magnitude': {'EAR': 4.784571387972392e-07, 'PUC': 2.892115844588261e-06, 'MAR': 5.346513987092294e-08, 'MOE': 1.3120636310759437e-07}, 'normalized': {'EAR': 0.13457780640812, 'PUC': 0.8134785222794995, 'MAR': 0.015038382040279935, 'MOE': 0.03690500799757159}, 'base_features': array([0.2252123 , 0.82143927, 0.02066149, 0.09174228], dtype=float32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Integrate Mistral explanation (paste after you have `result` and `prob`)\n",
        "# -------------------------------\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "# --- CONFIG: set your API key (better: pull from env var) ---\n",
        "API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"xGbwkJFTpe7BpsA0iyH462sYW8QPXFNs\")  # replace or set env var\n",
        "MODEL = \"mistral-tiny\"   # or mistral-small / mistral-medium\n",
        "API_URL = \"https://api.mistral.ai/v1/chat/completions\"\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# --- Safety checks ---\n",
        "if API_KEY is None or API_KEY == \"\" or \"your_api_key\" in API_KEY.lower():\n",
        "    raise ValueError(\"Please set API_KEY (put your Mistral key in MISTRAL_API_KEY env var or replace the placeholder).\")\n",
        "\n",
        "# --- Derive prediction label (use prob or pred_class if you have it) ---\n",
        "# If you have pred_class as a binary label, you can use that. Otherwise use prob threshold.\n",
        "# try:\n",
        "#     is_drowsy = None\n",
        "#     # prefer pred_class if available\n",
        "#     if 'pred_class' in globals():\n",
        "#         try:\n",
        "#             # pred_class may be shape (1,1) or (1,)\n",
        "#             is_drowsy = bool(int(np.ravel(state)[0]) == 1)\n",
        "#         except Exception:\n",
        "#             is_drowsy = None\n",
        "#     if is_drowsy is None:\n",
        "#         # fallback to prob (assumes prob in 0..1)\n",
        "#         is_drowsy = float(prob) > 0.4\n",
        "# except Exception:\n",
        "#     # final fallback\n",
        "#     is_drowsy = False\n",
        "\n",
        "# pred_label = \"Drowsy\" if is_drowsy else \"Alert\"\n",
        "\n",
        "# is_drowsy = stage != \"alert\"\n",
        "\n",
        "# --- Build importance dict from `result` ---\n",
        "# Prefer using absolute magnitudes to explain which regions contributed most\n",
        "if 'result' not in globals():\n",
        "    raise RuntimeError(\"Missing `result` variable. Run the feature importance routine first.\")\n",
        "\n",
        "# Use magnitude if available, else use absolute of raw\n",
        "if 'magnitude' in result:\n",
        "    importance_map = result['magnitude']\n",
        "else:\n",
        "    importance_map = {k: abs(v) for k, v in result.get('raw', {}).items()}\n",
        "\n",
        "# Keep ordering consistent\n",
        "feature_order = ['EAR', 'PUC', 'MAR', 'MOE']\n",
        "vals = np.array([importance_map.get(k, 0.0) for k in feature_order], dtype=float)\n",
        "\n",
        "# Normalize (for scaling in prompt only)\n",
        "vals_norm = vals / (vals.max() + 1e-12)\n",
        "\n",
        "# Prepare short textual summary (numbers included only for internal clarity; the assistant is instructed not to read them aloud)\n",
        "importance_text = \"\\n\".join([f\"{k}: {float(importance_map.get(k,0.0)):.6e}\" for k in feature_order])\n",
        "\n",
        "# -------------------------------\n",
        "# Build Mistral prompt\n",
        "# -------------------------------\n",
        "prompt = f\"\"\"\n",
        "You are an empathetic AI driving assistant that explains driver alertness.\n",
        "\n",
        "Prediction: {state}\n",
        "Prediction Probability : {prob}\n",
        "\n",
        "Feature importance values (absolute, gradient-based):\n",
        "{importance_text}\n",
        "These values are from the drowsiness classifier explaining how much weight each facial cue gave for the classification.\n",
        "the classifer works like:\n",
        "0.0 - 0.4 : alert(means not drowsy)\n",
        "0.41 - 0.7 : slightly drowsy\n",
        "0.71 - 0.85 : very drowsy\n",
        "0.86 - 1.00 : critical drowsiness\n",
        "\n",
        "mention the state of the driver first through finding the stage by putting the prediction probability into the classifier values, remember if the value is near the border for example 0.3956 it is still alert and not drowsy, if the valeu is less than 0.4 say the driver is not drowsy and alert\n",
        "\n",
        "if alert stop here else if drowsy, Explain to the driver what this means in a friendly and understanding way.\n",
        "If they are drowsy, warn them depending on the stage of drowsiness decided by the if else logic and the facial regions the system found indicative of drowsiness aka the regions with high values\n",
        "depending on the stage change the meesage for exmaple if crtitcal make the message more urgent. Incase the system is making a false prediction tell him to press the cancel button\n",
        "Keep it short, human, and supportive.\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# Query function\n",
        "# -------------------------------\n",
        "def mistral_chat(prompt, model=MODEL, api_url=API_URL, headers=HEADERS, timeout=15):\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.post(api_url, headers=headers, json=data, timeout=timeout)\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Request error:\", str(e))\n",
        "        return None\n",
        "\n",
        "    if resp.status_code != 200:\n",
        "        print(\"‚ùå API Error\", resp.status_code, resp.text)\n",
        "        return None\n",
        "    try:\n",
        "        payload = resp.json()\n",
        "        # Defensive: navigate response shape\n",
        "        content = payload[\"choices\"][0][\"message\"][\"content\"]\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Response parsing error:\", str(e))\n",
        "        print(\"Raw response:\", resp.text[:1000])\n",
        "        return None\n",
        "\n",
        "# -------------------------------\n",
        "# Call Mistral and print result\n",
        "# -------------------------------\n",
        "assistant_text = mistral_chat(prompt)\n",
        "if assistant_text is None:\n",
        "    print(\"No assistant response received.\")\n",
        "else:\n",
        "    print(\"ü§ñ Driving assistant message:\\n\")\n",
        "    print(assistant_text)\n"
      ],
      "metadata": {
        "id": "7KnV6Ne0HVyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db1ddd5-0afd-4056-a160-0fae7995de00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Driving assistant message:\n",
            "\n",
            "Based on the prediction, you seem to be in a state of high drowsiness, with a probability of 0.7011. This means you might be feeling sleepy and less alert than usual. Don't worry, it happens to us all! However, it's important to stay focused while driving. Let's break down the facial cues that are contributing to this classification:\n",
            "\n",
            "* EAR (Eye Aspect Ratio) - 4.78e-07 - This suggests your eyes may be closing slightly more than usual, which is a common sign of drowsiness.\n",
            "* PUC (Periocular Closure) - 2.89e-06 - This could mean your eyelids are drooping slightly, another indicator of drowsiness.\n",
            "* MAR (Mouth Aspect Ratio) - 5.35e-08 - This feature seems to have little impact on the classification, suggesting your mouth may not be a significant factor in your current drowsiness.\n",
            "* MOE (Magnitude of Excursion) - 1.31e-07 - This feature measures the movement of your eyes, and a low value could mean your eyes are moving less than usual, which can be a sign of drowsiness.\n",
            "\n",
            "Please take a moment to rest your eyes and stretch if you can. If you feel safe to do so, consider pulling over to a safe location to take a short break. Remember, it's better to rest for a moment than to risk becoming less alert while driving. If you feel that the system is making a false prediction, please press the cancel button. Stay safe on the road!\n"
          ]
        }
      ]
    }
  ]
}